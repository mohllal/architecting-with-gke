# Module 9: Big Data and Machine Learning in the Cloud

- ***Apache Hadoop*** is an open source framework for big data. It is based on the ***MapReduce programming model*** which Google invented and published.
- The ***MapReduce model*** is, at its simplest, means that one function, traditionally called the ***Map function***, runs in parallel with a massive dataset to produce intermediate results. And another function, traditionally called the ***Reduce function***, builds a final result set based on all those intermediate results.
- ***Cloud Dataproc*** is a fast, easy, managed way to run Hadoop, Spark, Hive, and Pig on Google Cloud Platform.
- Running on-premises, Hadoop jobs requires a *capital hardware investment*. Running these jobs in Cloud Dataproc, allows to *only pay for hardware resources used during the life of the cluster*. Although the rate for pricing is based on the hour, ***Cloud Dataproc is billed by the second***.
- ***Cloud Dataflow*** is both a unified programming model and a managed service and it lets you develop and execute a big range of data processing patterns: extract, transform, and load batch computation and continuous computation.
- Dataflow pipeline reads data from a big query table, the ***Source***, processes it in a variety of ways, the ***Transforms***, and writes its output to a cloud storage, the ***Sink***.
- ***BigQuery*** is Google's fully-managed, petabyte-scale, low-cost analytics data warehouse.
- ***Cloud Pub/Sub*** is meant to serve as a *simple*, *reliable*, *scalable* foundation for stream analytics. It can be used to let independent applications *send* and *receive* messages. That way they're decoupled, so they scale independently.
- The ***Pub*** in Pub/Sub is short for *publishers* and ***Sub*** is short for *subscribers*. Applications can *publish messages* in Pub/Sub and one or more *subscribers receive them*. Receiving messages doesn't have to be synchronous. That's what makes Pub/Sub great for decoupling systems.
- ***Cloud Datalab*** runs in a Compute Engine virtual machine. To get started, you specify the virtual machine type you want and what GCP region it should run in. When it launches, it presents an interactive Python environment that's ready to use. And it orchestrates multiple GCP services automatically, so you can focus on exploring your data.
- Machine learning is one branch of the field of artificial intelligence. It's a way of solving problems without explicitly coding the solution. Instead, human coders build systems that improve themselves over time through repeated exposure to sample data, which we call training data.
- The ***Cloud Vision API*** enables developers to understand the content of an image. It quickly classifies images into thousands of categories - sailboat, lion, Eiffel Tower - detects individual objects within images, and finds and reads printed words contained within images.
- The ***Cloud Speech API*** enables developers to convert audio to text. Because you have an increasingly global user base, The API recognizes over 80 languages and variants.
- The ***Cloud Natural Language API*** offers a variety of natural language understanding technologies to developers. It can do syntax analysis, breaking down sentences supplied by our users into tokens, identify the nouns, verbs, adjectives, and other parts of speech and figure out the relationships among the words. It can do entity recognition. In other words, it can parse text and flag mentions of people, organizations, locations, events, products, and media.
- The ***Cloud Translation API*** provides a simple, programmatic interface for translating an arbitrary string into a supported language.
- The ***Cloud Video Intelligence API*** lets you annotate videos in a variety of formats.