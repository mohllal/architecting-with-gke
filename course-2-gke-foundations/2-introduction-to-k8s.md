# Module 2: Introduction to Containers and Kubernetes

- A ***hypervisor*** is the software layer that *breaks the dependencies of an operating system with its underlying hardware*, and allow several virtual machines to share that same hardware. ***KVM*** is one well-known hypervisor.
- Running multiple applications within a single VM creates another tricky problem, ***applications that share dependencies are not isolated*** from each other, the resource requirements from one application, can starve out other applications of the resources that they need. Also, a dependency upgrade for one application might cause another to simply stop working. The VM-centric way to solve this problem is to run ***a dedicated virtual machine for each application***. Each application maintains its own dependencies, and the kernel is isolated.
- A more efficient way to resolve the applications dependencies problem is to implement ***abstraction at the level of the application and its dependencies (user space)***. The user space is all the code that resides above the kernel, and includes the applications and their dependencies. This is what it means to create containers. Containers are ***isolated user spaces per running application code***.
- Containers are ***lightweight*** because they don't carry a full operating system, they can be ***scheduled or packed tightly*** onto the underlying system, which is very efficient. They can be created and shut down very ***quickly*** because we're just starting and stopping the processes that make up the application and not booting up an entire VM and initializing an operating system for each application.
- Containers appeal to developers because they're an ***application-centric*** way to deliver high performance and scalable applications. Containers also allow developers to ***safely make assumptions about the underlying hardware and software***.
- An application and its dependencies are called an ***image***. A ***container*** is simply a running instance of an image.
- Containers use a varied set of Linux Technologies:
  - Containers use ***Linux process*** which has its own virtual memory *address space separate* from all others. And Linux processes are rapidly created, and destroyed.
  - Containers use ***Linux namespaces*** to *control what an application can see*. Process ID numbers, directory trees, IP addresses and more.
  - Containers use ***Linux cgroups*** to *control what an application can use*. Its maximum consumption of CPU time, memory, IO bandwidth, and other resources.
  - Containers use ***union file systems*** to efficiently encapsulate applications, and their dependencies into a set of clean minimal layers.
- A container image is structured in ***layers***. Instructions are read from a file called the ***container manifest***. In the case of a Docker-formatted container image, that's called a ***Dockerfile***. *Each instruction in the Dockerfile specifies a layer* inside the container image. Each layer is read-only. When a container runs from this image, it will also have a writable ephemeral topmost layer.
- These days, the best practice is ***not to build out application in the very same container that we ship and run***. After all, our build tools are at best, just *cluttered in deployed container*, and at worst, are an additional attack surface. Today, application packaging relies on a ***multi-stage build process*** in which *one container builds* the final executable image. And *a separate container receives only what's needed to actually run the application*.
- When we launch a new container from an image, the container runtime adds a ***new writable layer*** on the top of the underlying layers. This layer is often called the ***container layer***. All changes made to the running container, such as writing new files, modifying existing files and deleting files ***are written to this thin writable container layer***. In the ephemeral, *when the container is deleted the contents of this writable layer are lost forever*. The underlying container image itself remains unchanged.
- Google provides a managed service for building containers that's also integrated with Cloud IAM. This service is called ***Cloud Build*** Cloud Build can retrieve the source code for builds from a variety of different storage locations. *Cloud source repositories*, *Cloud storage*, or *git compatible repositories* like GitHub and Bitbucket.
- ***Kubernetes*** is an open source platform that helps us ***orchestrate*** and ***manage*** our container infrastructure On-premises or in the Cloud. It's a ***container centric management environment***. Google originated it and then donated it to the open source community.
- Kubernetes ***automates*** the *deployment scaling*, *load balancing*, *logging*, *monitoring*, and other management features of containerized applications. These are the features that are characteristic of a typical *platform as service solutions*. Kubernetes also facilitates the features of an *infrastructure as a service*, such as ***allowing a wide range of user preferences and configuration*** flexibility.
- Kubernetes supports declarative configurations Declarative configuration saves a lot work. Because *the system desired state is always documented*, it also *reduces the risk of error*. *Kubernetes also allows imperative configuration* in which we issue commands to change the system state. But administering Kubernetes as scale imperatively, will be a big missed opportunity.
- Kubernetes supports *different workload types*. It supports ***stateless*** applications such as an Nginx or Apache web server, and ***stateful*** applications where user in session data can be stored persistently. It also supports *batched jobs* and *demon tasks*. 
- Kubernetes can automatically ***scale in and out*** containerized applications based on resource utilization.
- We can specify resource ***requests levels and resource limits*** for our workloads and Kubernetes will obey them.
- Developers ***extend*** Kubernetes through a rich ecosystem of plugins and add-ons.
- Kubernetes also supports workload ***portability*** across On-premises or multiple Cloud service providers such as GCP and others.
- Google Cloud's managed service offering for Kubernetes, it's called ***Google Kubernetes Engine or GKE***. More specifically, GKE is a component of the GCP *compute offerings*. It makes it easy to bring our Kubernetes workloads into the Cloud. GKE is fully managed, which means that ***we don't have to provision the underlying resources***. ***GKE uses a container optimized operating system***. These operating systems are maintained by Google, and are optimized to *scale quickly and with a minimal resource footprint*.
- ***GKE's auto upgrade feature*** can be enabled to ensure that our clusters are automatically upgraded with the latest and greatest version of Kubernetes.
- ***GKE's auto repair feature*** can automatically repair unhealthy nodes for us. *It'll make periodic health checks on each node* in the cluster. If a node is determined to be unhealthy and requires repair, *GKE would drain the node*. In other words, ***it'll cause it's workloads to gracefully exit and then recreate that node***.
- Just as Kubernetes support scaling workloads, ***GKE supports scaling the cluster itself***.
- GKE ***seamlessly integrates*** with Google Cloud Build and Container Registry.
- ***Stackdriver*** is Google Cloud's system for monitoring and management for services, containers, applications, and infrastructure. ***GKE integrates with Stackdriver monitoring*** to help us understand our application's performance.
- GKE is ***integrated with Google Virtual Private Clouds or VPCs***, and makes use of GCP's networking features.
- ***Cloud Run*** is a managed compute platform that enables us to run ***stateless Containers*** via built *web requests* or *Cloud Pub/Sub events*. ***Cloud Run is serverless***. It's built on ***Knative***, an open-source Kubernetes-based platform. It *builds*, *deploys*, and *manages* modern serverless workloads.
- ***Knative***, an open API and runtime environment built on top of Kubernetes. It gives us the freedom to *move our workloads across different environments and platforms*, either fully managed on GCP, on GKE or anywhere Knative runs.
- 
