# Module 2: Deployments, Jobs, and Scaling

- ***Deployments*** describe a ***desired state*** of pods. The desired state is described in a deployment `yaml` file containing the characteristics of the pods. Coupled with how to operationally run these pods and handle their life cycle events.
- The ***controller*** is a *loop processes* created by Kubernetes that takes care of routine tasks to ensure the desires state of an object or a set of objects running on a cluster matches the observed state. During this process, a replica set is created. A ***replica set*** is a controller that ensures that a *certain number of pod replicas are running at any given time*. The deployment is a high level controller for a pod to declares that state. The deployment configures a replica set controller to instantiate and maintain a specific version of the pods specified in the deployment.
- Every time an update to the specification of the pods happens, for example updating them to use a newer container image, a ***newer replica set is created*** that matches the altered version of the deployment. This is how deployments ***roll out*** updated pods in a controlled manner. *All pods from the old replica set are replaced with newer pods and a new replica set*. If the updated pods are not stable, the administrator can ***roll back*** the pod to a previous deployment revision.
- Deployments are designed for ***stateless*** applications. Stateless applications don't store data or application state to a cluster or to persistent storage.
- The replica set created by the deployment ensure that the desired number of pods are running, and always available at any given time. If a pod fails or is evicted, the replica set automatically launches a new pod.
- `kubectl get deployment [DEPLOYMENT_NAME]` shows four states for the deployment replica set:
  - ***Desired***: shows the *desired number* of replicas in the deployment specification.
  - ***Current***: shows the number of replicas *currently running*.
  - ***Up-to-date***: shows the number of replicas that are fully *up-to-date as per the current deployment specification*.
  - ***Available***: shows the number of *replicas available to the users*.
- The Kubernetes ***service*** is a ***static IP address*** that represents a running service or a function. It's a *network abstraction for a set of pods* that deliver that service in, it *hides the ephemeral nature of the individual pods*.
- `kubectl autoscale deployment [DEPLOYMENT_NAME] --min=[NUMBER] --max=[NUMBER] --cpu-percent=[PERCENTAGE_THRESHOLD]` also auto-scales the deployment by specifying the minimum and maximum number of desired pods along with the CPU utilization threshold. This leads to the creation of a Kubernetes object called ***horizontal pod auto scaler***. This object performs the actual scaling to match the target CPU utilization.
- ***Thrashing*** sounds bad and it is bad. ***It's a phenomenon where the number of deployed replicas frequently fluctuate because the metric we use to control scaling also frequently fluctuates***. The horizontal pod auto-scaler supports a cool down or delay feature. It allows us to specify as a *wait period before performing another scale down action*. The default value is five minutes.
- There are many ways to update a Kubernetes deployment:
  - Using a `kubectl apply` command with an updated deployments specification `YAML` file.
  - Using a `kubectl set` command to change the pod template, specifications for the deployment such as the image, resources, or selector values.
  - Using the `kubectl edit` command to open a specification file using the Vim editor that allows us to make changes directly.
  - Using the GCP Console.
- When a deployment is updated it launches a ***new ReplicaSet*** and creates a new set of Pods in a controlled fashion. This also known as a ***rampt strategy***. It's advantage is that updates are *slowly release*, which ensures the *availability* of the application.
  1. First, ***new Pods*** are launched in a ***new ReplicaSet***.
  2. Next, all ***old Pods*** are deleted one by one from the ***old ReplicaSet***.
- In a rolling update strategy, the ***max unavailable*** and ***max surge*** fields control how the pods are updated. And there are a few additional options ,such as ***minready seconds*** and ***progress deadline seconds***.
  - The ***max unavailable*** field specify ***the maximum number of pods that can be unavailable during the rollout process***. This number can either be absolute or a percentage. The default max unavailable is ***25%***.
  - ***max surge*** specify the ***maximum number of pods that can be created concurrently*** in a new replica set. The default maxSurge is ***25%***.
  - ***minready seconds*** defines the ***number of seconds to wait before the pod is considered available*** without crashing any of its containers. The default for min ready seconds is ***zero***, meaning that as soon as the pod is ready, it is made available.
  - ***progress deadline seconds*** specify the *wait period before a deployment reports that it has failed to progress*.
- The ***blue/green deployment strategy*** is useful when we want to *deploy a new version of an application, and also ensure that application services remain available while the deployment is updated*.
- Services allow managing the network traffic flows through a selection of pods. This set of pods is selected using a ***label selector***. e.g. changing the service selectors to point at the ***new green deployment*** with the *v2 version label* instead of the ***old blue deployment*** pods that have the *v1 version label.*
- The ***canary deployment strategy*** method is another update strategy based on the blue-green method but ***traffic is gradually shifted to the new version***. The main advantages of using canary deployments are that we can minimize excess research usage during the update and because the roll-out is gradual, issues can be identified before they affect all instances of the application.
- In canary update strategy, the service selector is based only on a ***common label for the v1 and v2 of application deployments***. Initially, the new version (v2) of the deployment will start with zero replicas running. ***Over time as a new version is scaled up, the old version (v1) of the deployment can be scaled down, and eventually deleted***.
- ***Recreate*** is a strategy type where all the old pods are deleted before new pods are created. This clearly affects the availability of your application, because the new pods must be created and will not be available instantly.
- `kubectl rollout undo deployment [DEPLOYMENT_NAME]` command will revert the deployment to its previous revision. Or `kubectl rollout undo deployment [DEPLOYMENT_NAME] --to-revision=[REVISION_NUMBER]` to roll-back to a specific revision number.
- `kubectl rollout undo deployment [DEPLOYMENT_NAME]` shows the roll-out history. By default, the details of ***10 previous replica sets revisions*** are retained so that we can roll back to them. We can change this default by specifying a ***revision history limit***, under the deployment specification.
- Any deployment has ***three different lifecycle states***.
  - ***Progressing state***: indicates that a task is being performed, what tasks? Creating a new replica set, or scaling up or scaling down a replica set.
  - ***Complete state***: indicates that all the replicas have been updated to the latest version and are available and no old replicas are running.
  - ***Failed state***: occurs when the creation of a new replica set could not be completed. Why might that happen? Maybe Kubernetes couldn't pull the images for the new pods, or maybe there wasn't enough of some resource quota to complete the operation, or maybe the user who lost the operation lacks permissions.
- In an environment where small fixes are released frequently, we will have a large number of rollouts. To ***temporarily pause*** these roll-outs by using the `kubectl rollout pause deployment [DEPLOYMENT_NAME]` command. The initial state of the deployment prior to pausing will continue its function. ***But new updates suite to the deployment will not have any effect where the rollout is paused***. The changes will only be executed once a rollout is resumed. ***When resuming the rollout, all these new changes will be rolled out with a single revision***.
- A ***job*** creates one or more pods to run a specific task reliably. In its simplest form, a job will create one pod and track the task completion within that pod. *When the task is completed, it will terminate the pod and then report that the job has successfully finished*. Unlike other Kubernetes controllers, ***jobs manages a task up to its completion rather than to an open-ended desired state***. In a way, the desired state of a job is its completion. Kubernetes will make sure it reaches that state successfully.
- There are two main ways to define a job. ***Non-parallel and parallel***.
  - ***Non-parallel jobs*** create only one pod at a time. Of course, that pod is recreated if it terminates unsuccessfully. These jobs are completed when the *pod terminates successfully* or if *a completion counter's defined*, when the required number of completions is reached.
  - ***Parallel jobs*** are jobs that have a parallelism value defined where multiple pods are scheduled to work on the job at the same time. If they also have a completion count defined, there're used for tasks that must be completed more than once.
- Kubernetes considers parallel jobs complete when the number of pods that had terminated successfully reaches the completion count.
- The job ***restart policy*** can be:
  - ***Never***: This means that if a container in a pod fails for any reason, the entire pod fails and the job controller will respond to this pod failure by launching a new pod.
  - ***OnFailure***: In this case, the pod remains on the node but the container's restarted.
- ***BackoffLimits*** specifies the number of retries before a job is considered to have failed entirely. ***The default is six***. ***Failed pods are recreated with an exponentially increasing delay***: 10 seconds, 20 seconds, 40 seconds, and so on up to a maximum of six minutes.
- The parallel job type creates multiple pods that work on the same task at the same time. Parallel job types are specified by setting the `spec.parallelism` value for a job greater than one. There are two types of parallel jobs, one with a *fixed task completion count*, and the other which *processes a work queue*.
- In a fixed task completion parallel job, the job controller ***will only launch the maximum number of pods at the same time specified by the parallelism value***, and will continue restarting pods *until the completions count is reached*. If the remaining *number of completions is less than the parallelism value*, the controller will not schedule new pods, as there are sufficient remaining pods running at the point to complete the desired total for the job.
- In a worker queue parallel job, ***each pod works on several items from a queue and then exits when there are no more items***. Because the workers, the pods themselves attached when the work queue is empty and the job controller doesn't know about the work queue. ***It relies on the workers to signal when they're done working, by terminating***. At some point, when one of the pods terminates successfully. The applications running in the remaining pods detect this completion state and finish, causing the remaining pods to shut themselves down.
- ***Active deadline seconds*** setting sets an ***active deadline period*** for the job to finish. The deadline count starts when the job starts. If the deadline is reached, the job and all its pods are terminated with *a deadline exceeded reason*.
- ***CronJob is a Kubernetes object that creates jobs in repeatable manner to a defined schedule***. CronJobs are called that because they are named after Cron, the standard Unix Linux mechanism for scheduling a process.
- The ***concurrency policy*** value defines *whether concurrent executions are permitted*. With the values allow, ***forbid***, or ***replace***. In the case of forbid, if the existing job hasn't finished the CronJob won't execute a new job. With replace, the existing job will be replaced by the new job. This policy only applies to jobs that were created using the same CronJob.
- A ***node pool***, is a subset of node instances within a cluster. They all have the same configuration. Node pools use a ***NodeConfig*** specification. When you create a container cluster, the number and type of nodes that you specify, becomes a default node pool.
- Existing pools are not moved to the newer nodes, when the cluster size is increased.
- ***When reducing the size of a cluster, the nodes to be removed are selected randomly***. The resize process doesn't differentiate between nodes that are running pods, and the ones that are empty. ***When removing a node from the cluster, all the pods within that node will be terminated gracefully***. Graceful termination means that `unterm` signal is sent to the main process in each container. A grace period is then allowed before a kill signal is sent, and the pod is deleted. This grace period is defined for each pod. If these pods are managed by a replication controller, such as a replication set, or a StatefulSet, they'll be rescheduled on the remaining nodes. Otherwise, the pods won't be restarted elsewhere.
- `gcloud container clusters resize [CLUSTER_NAME] --node-pool [POOL_NAME] --size [POOL_SIZE]` command is used to decrease the size of a specific node pool.
- ***Cluster auto-scaling controls the number of worker nodes and response to the workload demands***. By default, the cluster auto-scaler is disabled. Cluster auto-scaling allows to pay only for resources that are needed at any given moment, and to automatically get additional resources when demand increases. ***When auto-scaling is enabled, GKE automatically adds a node to the cluster, if new pods are created and don't have enough capacity to run***. If a node in a cluster is underutilized, and it's pods can be run on other nodes, GKE can delete the node. Keep in mind that when nodes are deleted, the applications can experience some disruption. ***Before enabling auto-scaling, we should make sure that your services can tolerate the potential disruption***.
- ***When a pod has to wait for resource capacity, the scheduler marks the pod as unscheduled, by setting its schedule of pod condition to false with the reason, unscheduled pod***. If the auto-scaling is enabled, the GKE auto-scaler checks whether a scale up action will help the situation, as soon as it detects that any pod is considered unschedulable. If so, it adds a new node to the node pool, where the pod is waiting for resources to become available. The pod is then scheduled on that node. However, this requires a new VM instance to be deployed, which will need to start up and initialize before it can be used to schedule pods.
- If a node contains pods that meet an any of the following conditions, then the node can't be deleted:
  - ***Pods that are not managed by the controller***, these are Pods that are not set in a deployment, represents set, job, statements set, etc. Pods that have local storage.
  - ***Pods centered restricted by constraint rules that prevent them from running on any other node***.
  - ***Pods that have the safe-to-evict annotation `cluster-autoscaler.kubernetes.io/safe-to-evict` set to false***. The safe-to-evict annotation provides a direct sitting at the pod level that tells the auto-scalar that the pod cannot be evicted. As a result, the node that it's running on won't be selected for deletion when the cluster is scaled down.
  - ***Pods that have a restricted `PodDisruptionBudget` can also prevent a node from being deleted***. The `PodDisruptionBudget` specifies the number of controller replicas that must be available at any given time.
- ***At the node level, if the node scale-down-disabled `kubernetes.io/scale-down-disabled` annotation is set to True, that node will always be suited from scaled-down actions***.
- For each of the remaining nodes, the cluster auto-scalar adds up the total CPU and memory requests for the running pods. ***If this total is less than 50 percent of a node's allocatable capacity, the cluster auto-scalar will monitor that node for the next 10 minutes. If the total remains below 50 percent, the node is deleted***. After deleting a node, cluster auto-scalar re-analyzes a cluster to see whether additional nodes can be deleted.
- Some best practices for working with auto-scalar clusters.
  - ***Don't run Compute Engine autoscaling*** for managed instance groups on these nodes. The GKE auto-scalar is separate from Compute Engine autoscaling.
  - ***Don't manually resize a node pool*** using a gcloud command when the cluster auto-scalar is enabled. This might lead to cluster instability and result in the cluster having wrong node pool sizes.
  - ***Don't modify auto-scalar nodes manually***. All nodes in a node pool should have the same capacity, labels, and system pods.
  - ***Do specify correct resource requests for pods***. This will allow pods to work efficiently with the cluster auto-scalar. If you don't know the resource needs the pods to measure them under a test load.
  - ***Do use `PodDestructionBudgets`***. It's expected that the pods belonging to the controller can be safely terminated and relocated. If the application cannot tolerate such disruption, maintain the applications availability using `PodDestructionBudgets`.
- The cluster auto scaler has been tested to a maximum of ***1000 nodes*** with each running ***30 pods***.
- In Kubernetes, ***pod placement*** can be controlled to *labels* and *node affinity rules* and *toleration* in deployment specification.
- When containers have *resource requests specified the scheduler can make better decisions about which nodes to place pods on*. When containers have their *limits specified, contention for resources on a node can be handled in a specified manner*.
- A pod sums each containers resource, requests, and limits and sets up its own requested limits depending on the number of containers it's running. ***A scheduler assigns a pod to a node based on resource requests and limits set by the containers within the pod***. The scheduler ensures that a pod's requests and limits are within a node's capacity. It also spreads pods across nodes automatically. These nodes could be set up across different compute zones. ***When nodes are started, the `kublet` automatically assigns labels to them within a zone information***. Kubernetes will automatically spread the pods in a replication controller or a service across nodes in a single node, single zone cluster to reduce the impact of failures. With multiple zone clusters, the spreading behavior is it send it across zones to reduce the impact of zone failures.
- For a pod to run on a specific node, ***that node must match all the labels present under the `nodeSelector` field in a pod***. `nodeSelector` is a pod specification field that specifies one or more labels. The node labels may be automatically assigned. For example, the labeled `kubernetes.io/hostname` is automatically created by GKE. ***If the nodes labels are changed, running pods are not affected. Node selector is only used during pod scheduling***
- `nodeAffinity` also allows to constrain which nodes pod can be scheduled on based on labels. But the features are more expressive and it can be use to constrain against labels about nodes, another pods running on nodes.
- ***Unlike `nodeSelector` where a pod won't be scheduled if the `nodeSelector` requirements aren't met***, it's possible to define ***affinity*** and ***anti-affinity*** preferences that won't prevent a pod from being launched if the preferences aren't met.
- Affinity and anti-affinity are denoted by the `requiredDuringSchedulingIgnoredDuringExecution` and `preferredDuringSchedulingIgnoredDuringExecution` rules. The keywords contain the string ***IgnoredDuringExecution*** part to remind us that if the label is change, pods already running aren't affected. The ***requiredDuringScheduling*** rule shown here is a hard requirement, similar to `nodeSelector`. There must be met for the pod to be scheduled.
- We can add multiple `matchExpressions`. The node must satisfy ***all*** listed match expressions in each node selector. Logically, they are joined using ***Boolean And***.
- With the ***In*** operator, we can have multiple values, but ***only one*** is required to match. Logically, the In operator acts as a ***Boolean Or***. There are other operators such as *not in*, *exist*, *notexist*, *gt* for greater than, and *lt* for less than.
- ***When the scheduler evaluates pod placement preferences, each node that the pod might be scheduled on, receives a total weight score, based on all the requirements it meets***. Such as *resource requests*, *resource limits*, and other *node affinity rules*, such as `requiredDuringScheduling`, `ignoredDuringExecution`. The weight of `preferredDuringSchedulingIgnoredDuringExecution` is also added to this total score. ***The scheduler then assigns the pod to the node with the highest total score***. The weight defines the intensity of the preference. The value for the weight we can set for each rule *ranges from one, the weakest preference level to 100 for the highest preference*.
- ***Inter-pod affinity and anti-affinity features extend the node affinity concept to include rules based on pod labels that are already running on the node***. And instead of on labels of a node themselves. A pod that is required over first to run on the same node as other pods can be configure about pod affinity rules. ***Pods that most not or should not be scheduled on the same node as other pods can be configured with pod anti-affinity rules***.
- Using `topologyKey` also specify *affinity and non-affinity rules at a higher level than just specific nodes*. For example, to ensure that pods are not co-located in the same zone, not just the same node, we define a topology key to specify a pod anti-infinity rule should apply at the zone topology level. We can use `topologyKey` to specify topology domains such as node, zone, and region.
- ***Note affinity attracts Pods and anti-affinity repels them***.
- ***Taints*** are preventing Pods from being scheduled on specific nodes like anti-affinity. By contrast, we can figure ***taints on nodes***, and they apply to all Pods in the cluster.
- To taint a node, use the `kubectl taint nodes [NODE_NAME] key=value:[EFFECT]` command. ***The taint has a key with a value and a taint effect***. One taint effect example is the `NoSchedule` effect, which limits all possible scheduling on this particular node and all running Pods will be evicted.
- ***Tolerations*** are applied to Pods. ***A toleration is a mechanism that allows a Pod to counteract the effect of a taint that would otherwise prevent the Pod from being scheduled or continue to run on at node***. A toleration field consists of a *key*, *value*, *effect*, and *operator*. A Pod's toleration will match taint if the keys in the toleration and the taint are the same, the effects are the same, and the operator accepts the values. The operator field allows some flexibility. When an operator field is equal, the value must also be equal. However, when an operator field set to exist, only the keys and the effects must match for the toleration to apply even if the value field isn't specified.
- Three effects settings can be applied:
  - `NoSchedule` is a scheduling ***hard limit*** that prevents scheduling and Pods, unless there is a Pod toleration with the `NoSchedule` effect that matches.
  - `PreferNoSchedule` is a scheduling ***soft limit*** try to not place a Pod that doesn't have matching Pod toleration for the nodes taint. But this is only a soft limit, and a Pod might still be scheduled.
  - `NoExecute` will evict running Pods from the nodes unless the Pods each have at least one matching toleration with the `NoExecute` effect.
- ***Helm*** is an open-source package manager for Kubernetes in the same way that *AppGet* and *YARN* are package managers for Linux. ***Charts*** are easily created, versioned, shared, and published Kubernetes resources. Charts manages the deployment of complex applications. Think of a chart as a *parameterized YAML template*. Helm charts know what parameters are needed to make them work.
- In the Helm architecture, there are two components. First, there's a ***command line client***, which is also called *helm* in lowercase that allows us to develop new charts and manage chart repositories. The second component is the ***Helm server called Tiller*** which runs within the Kubernetes cluster. *Tiller interacts with the Kubernetes API server to install, upgrade, query, and removed Kubernetes resources*. It also stores the objects that represent a Helm chart releases.
- ***GCP Marketplace*** offers ready-to-go development stacks, solutions, and services to accelerate development of popular or open-source and commercial software packages.