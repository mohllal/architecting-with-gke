# Module 3: GKE Networking

- Each pod is assigned a *single IP address*, and the containers within a pod share the same network namespace, including that IP address.
- ***On a node, the pods are connected to each other through the node's root network namespace***, which ensures that Pods can find and reach each other on that VM. ***The root network namespace is connected to the node's primary NIC. Using the nodes VM NIC***, the root network namespace is able to forward traffic out of that node. This means that the IP addresses on the pods must be routable on the network that the node is connected to.
- ***In GKE, the nodes will get the pod IP addresses from address ranges assigned to the Virtual Private Cloud, or VPC***. VPCs are logically isolated networks that provide connectivity for resources you deploy within GCP, such as Kubernetes clusters, Compute Engine instances, and App Engine Flex instances. A VPC can be composed of many different IP subnets in regions all around the world.
- ***VPC native GKE clusters automatically create an alias IP range to reserve approximately 4000 IP addresses for cluster-wide services***.
- ***VPC-native GKE clusters automatically create a separate alias IP range for the pods. By default, the address range uses a slash 14 block, which contains over 250,000 IP addresses***.
- ***GKE allocates a much smaller slash 24 block to each node, which contains about 250 IP addresses***.
- GKE automatically configures the VPC to recognize this range of IP addresses as an authorized secondary subnet of IP addresses.
- In an ever-changing container amendments, ***services give pods a stable IP address and name that remains the same through updates, upgrades, scalability changes, and even pod failures***. A Kubernetes service is an object that creates a dynamic collection of IP addresses called ***endpoints*** that belong to pods matching the services labeled selector.
- There are several ways to find a service in GKE:
  - Environment Variables
  - Kubernetes DNS
  - Istio
- ***When new pod starts running on a node, `kubelet` adds a set of environment variables for each active service, in the same namespace as the pod***. If changes are made to a service after pods have been started, those changes will not be visible to pods that are already running. Pods will only see the changes that were made to the point where they are started, because the environment variables are defined inside the pod when the pods are started.
- In Kubernetes, DNS has an option add-on however, DNS is pre-installed in Google Kubernetes Engine. ***Kubernetes DNS server `kube-dns` watches the API server `api-server` for the creation of new services. When a new service has created, `kube-dns` automatically creates a set of DNS records for it***. Kubernetes is configured to use the `kube-dns` servers IP, to resolve DNS names for all pods. With this all the pods in the cluster can resolve Kubernetes service names automatically. By default, a client pods DNS search list, will include the pods, or name space, and the clusters default domain.
- ***A pod in any other namespace can resolve the IP address of the service, using the fully qualified domain name {name}.{namespace}.service.cluster.local***.
- `kube-dns` maintains the DNS record of the pods and services. ***To maintain high performance for service discovery, GKE auto scales `kube-dns`, based on the number of nodes in the cluster***. Every service defined in the cluster is assigned a DNS A record.
- Headless Kubernetes services. Those that are defined without a cluster IP also have DNS A and service records defined, but those results to the set of IP addresses of the pod selected by the servers.
- A service mesh provides an infrastructure layer that is configurable for micro-services applications. ***Istio*** is a service mesh to aid in service discovery, control, and visibility in the micro-services deployments.
- There are three principle types of services:
  - `ClusterIP`: A Kubernetes `ClusterIP` service has a ***static IP address***, and operates as a traffic distributor within the cluster. But `ClusterIP` services aren't accessible by resources outside the cluster.
  - `NodePort`: When creating a `NodePort` service, a `ClusterIP` service is automatically created in the process. ***This service can now be reached from outside of the cluster using the IP address of any node, and the corresponding node port number***.
  - `LoadBalancer`: When creating a `LoadBalancer` Service, ***GKE automatically provisions a GCP Network Load Balancer for inbound access to the services from outside the cluster***. Traffic will direct to the IP address of the network load balancer, and the load balancer forwards the traffic onto the nodes for the service.
- When creating a `ClusterIP` service, ***the cluster master assigns a virtual IP address, also known as cluster IP, from a resolved pool of Cluster IP address in the Cluster VPC***. This IP address won't change throughout the lifespan of this service. The cluster master selects pods to include in the service's endpoint based on the label selector and the labels on the pods.
- ***When the LoadBalancer Service is used, client traffic is directed through the network LoadBalancer to the nodes. The network LoadBalancer chooses a random node in the cluster and forwards the traffic to it***.
- There is a traditional trade of on the Kubernetes load balancing networking:
  - ***The lowest possible latency***.
  - ***The most even cluster load balancing***.
- ***When the lowest possible latency is most important, we can configure the `LoadBalancer` Service to force `kube-proxy` to choose a pod local to the node that receives the client traffic***. To do that, set the `externalTrafficPolicy: local` in the service manifest.
- The Ingress resource operates *one layer higher than the services*. In fact, it operates a bit like *service for services*. ***Ingress is not a service, or even a type of service. It's a collection of rules that direct external inbound connections to a set of services within the cluster***. ***In GKE, an ingress resource exposes those services using a single public IP address bound to an HTTP or HTTPS load balancer provisioned with GCP***.
- Ingress supports multiple host names for the same IP address. The traffic will be redirected from the HTTP or HTTPS load balancer based on the host names to their respective backend services.
- When the Ingress resource has been updated, the API server will tell the Ingress controller to reconfigure the HTTP or HTTPS load balancer according to the changes has been made.
- Ingress gains many security features from the underlying GCP resources it relies on. Ingress provides TLS termination support at the load balancer at the edge of the network. From there, the load balancer creates another connection to the destination. Although the second connection isn't secure by default, but it can be secured.
- `BackendConfig` is a custom resource used by ingress controller to define configuration for all these services.
- ***A regular HTTPS Load Balancer distributes traffic to all Nodes of an instance group, regardless of whether the traffic was intended for the Pods within that Node***. By default, a load balancer routes traffic to any Node within an instance group. ***The Network Load Balancer chooses a random node in the cluster and forwards the traffic to it. Next, to keep the Pod use as even as possible, the initial Node will use `kube-proxy` to select a Pod at random to handle the incoming traffic***. The selected Pod, might be on this Node or on another node in the cluster. *Therefore, this method has two levels of Load Balancing, one by the load balancer, and the other by `kube-proxy`, which results in multiple network hops*. ***This process does keep the Pod use even, but at the expense of increase latency and extra network traffic.*** In Kubernetes, the standard workaround for the double-hop problem, is to use the ***local*** external traffic policy.
- A true ***container first approach to load balancing***, is now available in GKE. The solutions still leverages the powerful GCP HTTPS load balancer. ***However, the load balancer now directs the traffic to the pods directly instead of the Nodes***. This method requires the GKE cluster, to operate in *VPC-native mode*, and it relies on a data model called ***Network Endpoint Groups or NEGs***. NEGs are a set of network endpoints representing IP to pod pairs which means that, Pods can't simply be just another endpoint, within that group, equal in standing, to compute instance VMs.
- There are many benefits to using Container-Native Load Balancing and Network Endpoint Groups:
  - ***Pods can be specified directly as endpoint for GCP load balancers***. The traffic is appropriately directed to the intended Pod eliminating extra network hops.
  - Load balancer features, such as ***traffic shaping*** in advanced algorithms are supported. With the direct connection to the Pod, the load balancer can accurately *distribute the traffic*.
  - Allowing direct visibility to the Pods, and more accurate health checks. Source IP is preserved, which provides visibility into round-trip time, for the client, to the load balancer, and helps with troubleshooting.
  - ***There are fewer network hops in the path, which optimizes the data path***. This improves latency and throughput, providing better network performance overall.
- A network policy is a ***set of firewall rules at the Pod-level*** that restrict access to other Pods and services inside the cluster. *By default, network policies are disabled in GKE*. ***There must be at least two nodes of `n1-standard-1` instance type or higher***. The recommended minimum is three. Network policies are not supported on `f1-micro` and `g1-small` instances.
- ***By default, enabling network policy enforcement for the cluster, also enables enforcement for the clusters nodes***. Enabling network policy enforcement consumes additional resources in nodes. This means, that we may need to increase the cluster size in order to keep the scheduled workloads running.
- In the `NetworkPolicy` resource policy, the `podSelector` enables to select a set of Pods based on labels. The `NetworkPolicy` is applied to these selected Pods. If `podSelector` isn't provided or is empty, the network policy will be applied to all Pods in the namespace. Policy types indicates whether `ingress`, `egress`, or both traffic restrictions will be applied. ***If policy type is left empty, a default `ingress` policy will apply automatically, and an `egress` policy won't be specified***.
- The network policy type `ingress` means ***incoming traffic*** to the affected Pods, just as `egress` means ***traffic going out*** of those Pods.
- *If an attacker would be able to probe outwards from the compromised Pod to all other Pods in the cluster. The attacker might find other vulnerabilities, it would be wise to limit the attack surface of the cluster*. ***Network policies let us lock down network traffic to only those pathways***.